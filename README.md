# Named-Entity-Recognition-Using-DistilBERT-HuggingFace
Built_a_Powerful_NER_Model_with_Hugging_Face_Transformers___Fine_Tuning_DistilBERT_For_NER___NER_with_Transformers
Named Entity Recognition (NER) Using Hugging Face Transformers

# Overview

This project demonstrates how to fine-tune a transformer-based model for Named Entity Recognition (NER) using the Hugging Face transformers library. The implementation is based on the Jupyter Notebook Build_a_Powerful_NER_Model_with_Hugging_Face_Transformers.

# Dataset

The notebook utilizes a structured NER dataset in CSV format.
The dataset contains text along with entity annotations.
The Entities column is pre-labeled with entity tags.

# DistilBERT
The DistilBERT model was proposed in the blog post Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT, and the paper DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than google-bert/bert-base-uncased, runs 60% faster while preserving over 95% of BERTâ€™s performances as measured on the GLUE language understanding benchmark.
